---
id: 6e4516ff-0701-4009-9841-b0f023ca43a6
title: LLM部署(docker+vllm+embedding+rerank) 支持工具调用
slug: 6e4516ff-0701-4009-9841-b0f023ca43a6
excerpt: 该文档介绍了关于LLM模型部署的内容，包括模型选择、模型下载、模型部署方案以及模型使用和加速方法。推荐的部署方案是使用docker部署，同时提供了本地环境部署的方法。模型加速方面介绍了vllm和flash-attention两种方法。embdding模型，rerank模型
date: 2024-06-03
coverImage: /images/6e4516ff-0701-4009-9841-b0f023ca43a6_7a2d1d060c9fb10000ed4af843e17828.png
lastUpdated: 2025-03-27T07:39:00.000Z
tags: orange:LLM  
---

# 模型选择


LLM模型，Embedding模型选择参考下面的文章


[link_to_page](https://www.notion.so/4ab81ed7-7622-4ef1-9fc6-1e1ae4edbd99)


# 模型下载


当前提供模型的网站主要有[ModelScope](https://www.modelscope.cn/models)和[HuggingFace](https://huggingface.co/models)，下载方式主要是git lfs和平台封装两种方法


## ModelScope

1. 安装modelscope

    ```shell
    pip install modelscope
    ```

2. 复制模型名称

    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/da864e11-683f-4c2d-a264-16ecdf57fff9/94c1b3e8-aeeb-4fdb-adf5-fedb3bad4f75/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466Z3ENBDO3%2F20250703%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250703T060218Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAYaCXVzLXdlc3QtMiJHMEUCIQDHdsbNptl57J2HbfJUF%2BnrNig6e%2BkWtXR7weRpE1IMzAIgBK8na%2BsqThRbujlK%2BvohqOxeAX4Bfn1ytRRUvCE4skkqiAQI%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw2Mzc0MjMxODM4MDUiDCOqBjPgJBAAALH0zSrcAzzQbN%2BObJw6rOPlpcVvWpRXlBO0PD96ntGH4RKPOLhB%2FCvm8h186XMrN0%2FSvBZqhkxjbL0GeAEKLzSA%2BZchbf%2Bkyyz658kSNfzTlm%2FN1q3w8eF4hGcnHodxZ1CzbMbUQjjjMNlCddPkhO%2FZEKDXELYv7F6C0QDO2EDKKRzEUreykUo8qY9gRFFiPp6RkMGkmNycUrKb5172hrVQEX%2Fuld92ldj%2B%2B7UIOqj51I6znkQt5nkjzyadCz3OTACCA3lAR5pcyiKPIt7u3EsytQ55HAUHkCCw%2FS241nstT9YIM0F3l8LE5Ayc0Z2BFmqJeGNhOkttJFN6XpgY87U4kv7MwSG0HyyKaoo0UDxSsXXgYSsZP%2BVn2qMqOSWUoZ74yES3h7ea6Hook6NaXkudHYibMVea5wkHiyZwqX5sqP5iG0DvTe3eTN0VUsIpGxuXY8MCgUOfiidcdK2fMemSsV3HPU8QMa1P5FPTU6Van%2BhUXpnW0ENDUkiMG0lqcDf19aM1HCo48jcFCri9G4CNqfGcN22n6qQi%2Ftn4XXLKASKf7QoLU4VBj7gtjQc4tmxCjHz0Q1Sua%2FQ32eIyNQ03hLcXzKuN3SO7%2FoSfgfmWCXgksm6JkJoAWsHWTr0mYj6zMMivmMMGOqUBF3Usz3v0uNJHRpwzCVt2fntV32zkmCOxm%2F4sHUW1bs7C0niQpGzgU%2B12q1T8Xmlr402Nnyovws6If%2FLQLIzZgqAnHNmayxcdgK7l9OnUN6vHmhiw%2BsL%2BZ9hxlqzm8fDA40NdbWygkwYzmeWLcnOAgTMMTpltWhHVNdHlQg31nfu1ATn4G1uM401JKdmfjJx38KHL3f%2FBwadPVGcdDYckRNz5jEF8&X-Amz-Signature=6647f3df3ec60f62b64995e66b71fcba06a19e67d2a685951b758828c7e22296&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

3. 下载模型到指定目录

    ```shell
    #模型下载
    from modelscope import snapshot_download
    # 注意替换模型名称，不指定目录，则默认下载到用户目录.cache/modelscope/
    model_dir = snapshot_download('qwen/Qwen2.5-72B-Instruct-GPTQ-Int8', cache_dir='/data/models/')
    ```


# 模型部署


## LLM、Embedding、Rerank docker部署

1. 安装docker，国内使用[清华开源软件镜像站](https://mirror.tuna.tsinghua.edu.cn/help/docker-ce/)
2. [安装](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)[**NVIDIA Container Toolkit**](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
3. 使用docker compose 部署，部署文件见下面的github地址

    [bookmark](https://github.com/bluechanel/deploy_llm/tree/main)

4. clone 项目

    ```json
    git clone git@github.com:bluechanel/deploy_llm.git
    cd deploy_llm
    ```

5. 修改模型保存目录

    ```yaml
    x-vllm-common:
      &common
      image: vllm/vllm-openai:latest
      restart: unless-stopped
      environment:
        TZ: "Asia/Shanghai"
      volumes:
        - /data/models:/models # 此处修改为实际模型目录。
      networks:
        - vllm
    ```

6. 修改模型启动参数

    vllm的更多参数见[vllm文档](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#cli-reference)

    1. LLM

        修改command 里面的 `—-model` 后面的模型目录，映射到docker中的目录


        ```yaml
        command: [ "--model","/models/{你的模型目录}",  "--enable-prefix-caching","--host", "0.0.0.0", "--port", "8000", "--served-model-name", "gpt-4", "--distributed-executor-backend","ray","--tensor-parallel-size","2","--pipeline-parallel-size", "1","--enable-reasoning","--reasoning-parser","deepseek_r1"]
        ```


        这里有几个常用参数说明


        `--served-model-name`：模型调用名称，可以自定义填写任意名称


        `--tensor-parallel-size`：并行数量，取决于使用的显卡数量
        `--enable-prefix-caching`：开启前缀缓存，对多轮对话有一定效率提升


        `"--enable-reasoning", "--reasoning-parser","deepseek_r1"` 如果是推理模型，可以配置该参数，目前支持`deepseek_r1`系列


        `"--enable-auto-tool-choice", "--tool-call-parser", "hermes”`：开启工具调用能力，例如Qwen2.5 系列模型，参考


        ```yaml
        command: [ "--model","/models/qwen/Qwen2___5-72B-Instruct-GPTQ-Int8", "--enable-prefix-caching", "--host", "0.0.0.0", "--port", "8000", "--served-model-name", "gpt-4", "--enable-auto-tool-choice", "--tool-call-parser", "hermes","--distributed-executor-backend","ray","--tensor-parallel-size","2","--pipeline-parallel-size", "1" ]
        ```

    2. Embedding

        修改command 里面的 `—-model` 后面的模型目录为映射到docker中的embedding模型目录


        ```yaml
        command: [ "--model","/models/{你的模型目录}",  "--host", "0.0.0.0", "--port", "8000", "--task", "embed", "--served-model-name", "gte-large-zh"]
        ```

    3. Rerank

        修改command 里面的 `—-model` 后面的模型目录为映射到docker中的reranker模型目录


        ```yaml
        command: [ "--model","/models/{你的模型目录}",  "--host", "0.0.0.0", "--port", "8000", "--task", "score", "--served-model-name", "bge-reranker-base"]
        ```

7. 使用docker compose 启动模型

    ```json
    docker compose up -d
    ```


    模型启动后，docker对外暴露在8000端口，访问`http://ip:8000/docs`查看接口文档

8. 测试，使用demo脚本测试。注意修改 各模型的自定义名称

    ```json
    python demo.py
    ```


## LLM、embedding、reranker分体部署

# LLM部署

1. clone 项目，并进入llm目录

    ```shell
    git clone https://github.com/bluechanel/deploy_llm.git
    cd deploy_llm/llm
    ```

2. 修改模型映射路径，`vim docker-compose.yaml`

    ```shell
    x-common:
      &common
      volumes:
      # 修改为自己下载模型的地址映射到容器/models
        - 
    /data/models:/models
    
      environment:
      # 时区设置
        &common-env
        TZ: "Asia/Shanghai"
    ```


    修改模型启动命令，在vllm服务中，修改`--served-model-name` 为自定义模型名称   `--model`为修改后的模型路径，`--tensor-parallel-size 4`为使用显卡数量，根据实际情况修改


    ```shell
    command: [ "--model","/models/qwen/Qwen2___5-72B-Instruct-GPTQ-Int8",  "--host", "0.0.0.0", "--port", "8000", "--served-model-name", "gpt-4", "--enable-auto-tool-choice", "--tool-call-parser", "hermes","--distributed-executor-backend","ray","--tensor-parallel-size","4","--pipeline-parallel-size", "1" ]
    ```

3. 启动`docker compose up -d`
4. 查看api文档`http://ip:1281/docs`

## Embedding+Rerank部署


> 💡 embedding 和 rerank是两个模型，可直接在modelscope搜索rerank找相关模型

1. 进入embedding目录
2. 修改模型映射路径，`vim docker-compose.yaml`

    ```shell
    x-common:
      &common
      volumes:
      # 修改为自己下载模型的地址映射到容器/models
        - 
    /data/models:/models
    
      environment:
      # 时区设置
        &common-env
        TZ: "Asia/Shanghai"
    ```


    修改embedding启动命令，修改`--model-id`为修改后的模型路径


    ```shell
    command: [ "--json-output", "--model-id", "/models/maple77/gte-large-zh"]
    ```

3. 启动`docker compose up -d`
4. 查看api文档embedding: `http://ip:1282/docs` rerank:`http://ip:1283/docs`

    ![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/da864e11-683f-4c2d-a264-16ecdf57fff9/e1756aaa-6b65-4e54-a5fe-aa2bba18033b/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XYIABD7R%2F20250703%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250703T060227Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAYaCXVzLXdlc3QtMiJHMEUCIQCVcMhgJcgu16nU7yF7BqHjHlasYaxClscBoxCu2TvxjwIgCw%2Bq9K0vs3JBSzN76NVcDlT5citzk3tFzoTxZ4q1vAcqiAQI%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw2Mzc0MjMxODM4MDUiDE4A5gwSMI8j4DFV8CrcA91UNHz998OFIxnse%2B8p5Y%2FkFzPLmIa7Ze22yEtG%2BxzPrdONYBasWZk1X9J%2Bx75vHXHMZnlmsm5wIVhFYSyjVptow7QbFT1npLBxvjIhL3Mip55HY4yLpCQ2xyokSBc5nB6M0Yuot4wkT9AxWuXPG1LwBfLtJssr%2Ftc7fOOwVPYn87GP5tut53L6A%2BdMFOywR6QSXNj30YaPe%2B96jUQY8YQBLvqTNVsL9NtsCPJB4Lb0Anf9MLraj8n6NmQ3HJ0ugPX9WKJd%2Bj6TUNSAWUyAndqfDZ3Sbji2bQNeH9eCli%2FVnBfGUhu0ZSUeNXt%2BqfwUWdY0rR9Z1%2Bg1Hs1qcIk7HfyxYb%2F1e%2FhvOPbjxOr%2Bbzs0v40BZvDg8hoLG%2BjRJS3sZ2KctgaotqMLukNMEUJlWmhvQOylxOGUoE3Bv3GuhkxWRsQ2FC%2B0GEH4q%2FjbcAzWK4mZNCxZ56lY%2FFATVIXWwlTAjW%2FSAFQ3ylYP0QmXpa69s5xramTOtnSiqZf%2BxZ6ul7ZEfbceARdEyZWfuan7aEZ3D056VHMb%2FmCCjNvWO%2FsjTJ3Ja4YQHz9pH5KwkMoJV1KXsyprvap%2Bn1CrP4bd%2Bx1EPV3FMuC2X%2FyhoSwCyutwaYaJg2SOoMBpX63vMKivmMMGOqUBt0t6Fb5b0jRhfC5bbU8j1bu%2BB%2BOg72rTIT0wsrA6UDlfuf2sQwLAZh6rKPYO5q3cdgQGDDls8j%2FNY6mIK5NROryOSb0uEVO9oxjspM0GIHFLL4C1oekhOyjMDk9HunNYk1IIez0aJlmg2oLEHcPRn4qRlueFmnjMJIaK6kI6%2BihhxN6rZBGDK9W1EgTELqfwkFBt1%2BaSAy%2FmP2bzPWaRrwM0ry16&X-Amz-Signature=b43f81c0901c0e14ff89c4997ff6e4b8168677174267f25f60175a1f7969b461&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)


**排错**


vllm启动可能会有如下报错，在docker compose中修改`shm_size`的值为错误提示的值，即可


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/da864e11-683f-4c2d-a264-16ecdf57fff9/bae86682-7725-440f-a248-074de305b696/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466ZUUDRH4W%2F20250703%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250703T060225Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAYaCXVzLXdlc3QtMiJGMEQCIC2mcAl%2FgptVsCfvqfVgGZqfdK42dRQJrjLHb4ywkC4TAiBRDGVBfWM9%2Fgglgyt1R66XWI8vjmxfZxjQ0Qf1vhZ4NiqIBAj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMe0%2FksyTa861UpSppKtwDUjIXxfuOUW2gHPUay3mNHdQG357CNxxAFUdi7bSPDOScllmtL58PvTtdPXsa8M3H%2BkPuNMZ4yLaVG%2F7a8mVOn7cl7DDYHJlsy%2FSmRCwo1UUsmkTewOPjDPus77G2wLhpjwiJ%2BgjgmQCZd0P4RdkdCdimChh7NrFO4rleIIeOcZGA7CNrj6SLE04jQ8jnG5bWDgb%2BZq2wgs%2BjEyW3Ht8nvC5B3knPrK3icvNNDUIbAwOTItn1ohqOBiVv9JDJIk0komdfbuchwwCHyJVq42ikYX0QA0s9717mIyMSvS%2Be0600pq0ZQ1k20T1PGnA5MLJ6oRBfaATDb2ClHh%2B9sEN0oHkLUtyU8cziiiPgCvdnu7tz0kzHq0XR4GmDMmHBzZ%2B5WqS4DbJ%2FcDS4gXssE7doij3D2DxA5J8m1V05dr6oS7gODO%2F6tKoH5HBLg4ECZJGX02PSmn5hgO7qbZ7vQNT4%2FXluzsY1%2FRKILgskHmNUIE0VVbse%2B4bgXzKP22PIE%2FDzI0j%2FdXspGkMG%2BtvF46H6Jrt3pIvWN1m3I9euo84otIdCWw8SyLHUisobzQa%2BsSC%2FoC5q6Q2YYyx3I4kvd69IIqmYTFko1o%2FnZ0bkzsPOBYopfl7Ffb6hMQvZskAwh6%2BYwwY6pgHPRmc1FTrn%2FaAr5mKyUch1y29Aw667cR3Z0jYOxWaSMme0hs1HgDGTouc6QTPCnD7Nss1ive8GpMoGH%2BTGizoRC5Aup4j3crunqkmiW03%2BhNwUnEWQNit2ZPozmUYjDvXEPlkPPRuneSacRZEzsL3UCDJURPOPK%2B9Xvd4IPqGV7lklrbYgh3Y7699RhPNbIiiLTm5oDKQlPurgX%2FjuOUj5TP%2BnHIPY&X-Amz-Signature=899c2d86d8312916a3aa759ba867806d4fcd9a99e2ad3be58ffcd7876689518e&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)


## 已废弃部署方法（20240918）

# 模型部署


当前开源的模型部署服务很多，主流的有[FastChat、](https://github.com/lm-sys/FastChat)[Xinference](https://github.com/xorbitsai/inference)、[ollama](https://github.com/ollama/ollama)、[vllm](https://github.com/vllm-project/vllm)、[lightllm](https://github.com/ModelTC/lightllm)，其中vllm，lightllm主要是用于**模型加速**。同时FastChat等也支持使用vllm启动模型获得高效加速，不过这些部署服务都**不支持工具调用**，也就是OpenAI 接口的tools参数。遂我对FastChat的代码做了部分修改，使其**支持tools参数。**具体代码见github，（仅测试了Qwen系列）


> 💡 由于不同模型训练数据不同，同样的Prompt在不同的模型中结果差异较大，导致tools能力不稳定，该能力未提交FastChat原始仓库。


[bookmark](https://github.com/bluechanel/FastChat/tree/main)


推荐的部署方案为：FastChat+vllm


## 方案1：docker部署(推荐)

1. 安装docker，国内使用[清华开源软件镜像站](https://mirror.tuna.tsinghua.edu.cn/help/docker-ce/)
2. [安装](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)[**NVIDIA Container Toolkit**](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
3. 使用docker compose 部署，部署文件见下面的github地址

    [bookmark](https://github.com/bluechanel/deploy_llm/tree/main)


### LLM部署

1. clone 项目，并进入llm目录

    ```shell
    git clone https://github.com/bluechanel/deploy_llm.git
    cd deploy_llm/llm
    ```

2. 修改模型映射路径，`vim docker-compose.yaml`

    ```shell
    x-common:
      &common
      volumes:
      # 修改为自己下载模型的地址映射到容器/models
        - 
    /data/models:/models
    
      environment:
      # 时区设置
        &common-env
        TZ: "Asia/Shanghai"
    ```


    修改模型启动命令，在`fastchat-model-worker`服务中，修改`--model-names` 为自定义模型名称   `--model-path`为修改后的模型路径，`"--num-gpus", "4"`为使用显卡数量，根据实际情况修改


    ```shell
    entrypoint: [ "python3", "-m", "fastchat.serve.vllm_worker", "--model-names", "gpt-4", "--model-path", "/models/qwen/Qwen2-72B-Instruct-GPTQ-Int8", "--worker-address", "http://fastchat-model-worker:21002", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "21002", "--num-gpus", "4" ]
    ```

3. 启动`docker compose up -d`

    **注意:**


    此版本Api接口使用的是支持**工具调用**的，如果不需要，请修改`docker-compose.yaml`文件中`fastchat-api-server`的启动命令为


    ```shell
    entrypoint: [ "python3", "-m", "fastchat.serve.openai_api_server", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "8000" ]
    ```

4. 查看api文档`http://ip:1281/docs`

    ![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/da864e11-683f-4c2d-a264-16ecdf57fff9/5813f5f8-9f74-497d-a7d3-ff6317a1e549/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4663H7AUIKQ%2F20250703%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250703T060231Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAYaCXVzLXdlc3QtMiJIMEYCIQCd%2BfKIuEIEPHnrn1LWGHY2Eek%2BaH0uQHQAu7AOkCeZVwIhAJo%2F2lcuVpKw%2BAgUANFWqahW%2BVh8HxbYIBnIJlh0qQrfKogECP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgzHimE%2Bk3AED7MscIMq3AP5Jm9EKUtWgcIUyr%2F8T6LwN9Htqpgr2cK5yGMhzPajRuWuMY1mN6q8tiqOSmmCwl51kv1AL5c9EV03V1B0TagdrpTz5uKpr0M2rYVJmHHmPHKnME9zAb3MgwjmqcPI6nIf6BOsuCk50awfju7rnqrzzTr7QxtwoJpcDEs1XxAyigDwR77yfsoWUf%2FKkPis0RdLSmOgtP656xamrLAGfEqgFEM7WNAOFRFHDHEn75cAp3DazH%2BDFa6N6dh%2BX%2FvxGwZblvU31eKqYBqzyaVHoPpKe%2BkY4V3%2F7vxQbJf%2FrnfmCgkvKvJ%2BTGEJ%2F76Ryyhcwd6kJd2pm3zrKTHVCg%2FPvSnH07B0JBpLhjYGcz%2B4j1aASKXzCkXov8PZEKomUQi7H%2Fz5ZuNWqrboMu90Te0SQg1S32wl%2BNJvLXJa6jNzJRYSjyvohyhTulGRbpPwKI64CBjBFkV5YzToaZX9rElAcUPF0cs7b543cyudzqws0rgm58NlaWKkiSTOiNKDn8D%2FICYGzup6iDJnz4br2MTriJN5tcYMxjuq0kjDOjiBfeNa%2FZsQShEUJMCKsahPDjtz7ZiLBL8RI%2BMWY9wf6LuDO1lrPHHjXZy%2FF4cNNsCPTxpYlDTXH836nXsWtjdCVDDGr5jDBjqkAcYcDB8zIapYkYPxoaK0%2FbKsN64SV%2Fmm%2Bj%2BHHXkS5wMOltJBDo5zVzs0fo9jiy75XRJf0odluMH8lyCIPX82fLdjMfCi7sklbRck2fo5yPXAsSrA%2FZI4ij%2FDqpa9ml%2FEgNc%2BRrH%2FSzYEnL3fiOZjYO6nG8UjTyvTnd%2FVRfFqnjBikrCT6a43BUCnkW9CM%2FQ8LcCDH5KEoOt%2FvrQ0s4ktb7fkdNa6&X-Amz-Signature=63fe4750de70e9b654c816e4c84a5318b241f81f1d473331a18967a1aa928345&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)


## 方案2：本地环境部署


使用fastchat加载模型（[支持模型](https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md)），由于LLM都是由transformers开发，理论上fschat可以用于启动所有LLM


[link_preview](https://github.com/lm-sys/FastChat)


```python
conda create -n fschat python=3.10

pip install fschat
```


命令行启动


```python
conda activate fschat
python -m fastchat.serve.cli --model-path /data/models/qwen/Qwen-14B-Chat
```


openai接口方式启动


```python
conda activate fschat
python -m fastchat.serve.controller
python -m fastchat.serve.model_worker --model-path /data/models/qwen/Qwen-14B-Chat
# 此处也可替换为使用vllm worker
# python -m fastchat.serve.vllm_worker --model-path /data/models/qwen/Qwen-14B-Chat
python -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 1282
```


### supervisor 管理


```python
# 由于启动项较多，我们使用supervisor管理
pip install supervisor
```


supervisor 配置文件`supervisord.conf`增加如下内容，并创建文件夹`/data/supervisor/conf.d`


```python
[include]
files = /data/supervisor/conf.d/*.conf
```


在`/data/supervisor/conf.d`中创建`llm.conf`,写入如下内容, 重点是llm_model的启动参数，model_path用于指定模型文件的地址，对于多GPU，添加参数`--num-gpus 4 --max-gpu-memory "80GiB"`


```python
[program:llm_ctrl]
command=/home/jx/anaconda3/envs/fschat/bin/python3 -m fastchat.serve.controller
stdout_logfile=/data/supervisor/logs/ctrl.log

[program:llm_model]
command=/home/jx/anaconda3/envs/fschat/bin/python3 -m fastchat.serve.model_worker --model-path /data/models/qwen/Qwen-14B-Chat --num-gpus 4 --max-gpu-memory "80GiB"
stdout_logfile=/data/supervisor/logs/model.log

[program:llm_api]
command=/home/jx/anaconda3/envs/fschat/bin/python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 1282
stdout_logfile=/data/supervisor/logs/api.log
```


# 模型使用


在langchian中套壳ChatOpenAI使用，或直接使用OpenAI SDK，可参考demo.py


### LLM


**方式1**


```shell
from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import SecretStr

class MyChat(ChatOpenAI):
    openai_api_base = "http://ip:1282/v1"
    openai_api_key = SecretStr("123456")
    model_name = "Qwen-14B-Chat"
    max_tokens = 1024# 依据不同模型支持的长度进行调整

llm=MyChat(temperature=0)
```


**方式2**


```python
os.environ.setdefault("OPENAI_API_KEY", "12123123")
os.environ.setdefault("OPENAI_API_BASE", "http://ip:1282/v1")
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name="Qwen-14B-Chat")
```


### Embedding


```shell
from langchain_openai import OpenAIEmbeddings
from pydantic.v1 import SecretStr


class TaliAPIEmbeddings(OpenAIEmbeddings):
    openai_api_base = "http://ip:1281/v1"
    openai_api_key = SecretStr("123456")
    check_embedding_ctx_length = False
```


# 模型加速

1. [vllm](https://github.com/vllm-project/vllm)
2. [flash-attention](https://github.com/Dao-AILab/flash-attention)

    安装遇到的问题：

    1. OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.

        指定cuda home地址


        `CUDA_HOME=/usr/local/cuda-11.8 python` [`setup.py`](http://setup.py/) `install`or`CUDA_HOME=/usr/local/cuda-11.8 pip install flash-attn --no-build-isolation`

