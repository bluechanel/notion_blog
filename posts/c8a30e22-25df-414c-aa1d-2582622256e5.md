---
id: c8a30e22-25df-414c-aa1d-2582622256e5
title: ğŸ¥  ä¿®æ”¹FastChatä½¿æ”¯æŒå·¥å…·è°ƒç”¨(LangGraphé€‚é…FastChat)
slug: c8a30e22-25df-414c-aa1d-2582622256e5
excerpt: æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä¿®æ”¹FastChatä»¥æ”¯æŒå·¥å…·è°ƒç”¨å’ŒLangGraphé€‚é…FastChatçš„é—®é¢˜ã€‚é’ˆå¯¹
date: Wed May 08 2024 08:00:00 GMT+0800 (ä¸­å›½æ ‡å‡†æ—¶é—´)
coverImage: /images/c8a30e22-25df-414c-aa1d-2582622256e5_fdcb66a6a0840f2ca7837acaad8e548b.png
lastUpdated: Thu Jun 20 2024 17:58:00 GMT+0800 (ä¸­å›½æ ‡å‡†æ—¶é—´)
tags: orange:LLM,green:LangChain,blue:LangGraph
readTime: 3.105
---

# å¯¼è¨€


é¦–å…ˆï¼Œéœ€è¦æ˜ç¡®çš„ä¸€ç‚¹æ˜¯**LangGraphæ˜¯åŸºäºLangChainå¼€å‘**ï¼Œè€Œå½“å‰LangChainæ”¯æŒçš„æ¨¡å‹**APIæœ‰é™**ï¼Œåªæœ‰openaiï¼Œanthropicï¼Œmistralaiç­‰å‡ ä¸ª(æˆªè‡³2024.5.8ï¼Œå‚è€ƒé“¾æ¥[https://github.com/langchain-ai/langchain/tree/master/libs/partners](https://github.com/langchain-ai/langchain/tree/master/libs/partners))ã€‚


æ‰€ä»¥å½“æˆ‘ä»¬ä½¿ç”¨**FastChat+vllm**åœ¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹åï¼Œå°†é¢ä¸´å¾ˆå¤šé€‚é…é—®é¢˜ï¼Œæœ¬æ–‡ä¸»è¦ç”¨äºè§£å†³è¿™äº›é—®é¢˜ã€‚


# `with_structured_output` return is None


`with_structured_output`æ–¹æ³•ç”¨äºç»“æ„åŒ–è¾“å‡º


## åŸå› 


æŸ¥çœ‹è¯¥æ–¹æ³•æºç ï¼Œå¯ä»¥æ˜ç¡®é—®é¢˜åŸå› ï¼Œæ˜¯ç”±äºä½¿ç”¨äº†openaiæ¥å£æ‰€æä¾›çš„`function_call` æˆ– `json_mode`ã€‚è€Œfastchatçš„ç±»openaiæ¥å£å¹¶æ²¡æœ‰æä¾›æ­¤å‚æ•°


![Untitled.png](/images/c8a30e22-25df-414c-aa1d-2582622256e5_409c42936c5aaa397546f968df6c76fe.png)


## è§£å†³æ–¹æ¡ˆ

- æ–¹æ¡ˆ1ï¼šä½¿ç”¨chainæ›¿æ¢è¯¥æ–¹æ³•

    åŸä½¿ç”¨æ–¹æ³•ï¼Œ[å‡ºå¤„ç¤ºä¾‹](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/)


    ```python
    class RouteQuery(BaseModel):
        """Route a user query to the most relevant datasource."""
    
        datasource: Literal["vectorstore", "web_search"] = Field(
            ...,
            description="Given a user question choose to route it to web search or a vectorstore.",
        )
    
    
    structured_llm_router = llm.with_structured_output(RouteQuery)
    ```


    ç°ä½¿ç”¨æ–¹æ³•


    ```python
    class RouteQuery(BaseModel):
        """Route a user query to the most relevant datasource."""
    
        datasource: Literal["vectorstore", "web_search"] = Field(
            ...,
            description="Given a user question choose to route it to web search or a vectorstore.",
        )
    
    
    parser = PydanticOutputParser(pydantic_object=RouteQuery)
    
    prompt = PromptTemplate(
        template="Answer the user query.\n{format_instructions}\n{query}\n",
        input_variables=["query"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    
    structured_llm_router = prompt | llm | parser
    ```

- æ–¹æ¡ˆ2ï¼šç»§æ‰¿ChatOpenAIï¼Œé‡å†™è¯¥æ–¹æ³•

    ```python
    from langchain_openai import ChatOpenAI
    class MyChat(ChatOpenAI):
        def with_structured_output(
                self,
                schema: Optional[_DictOrPydanticClass] = None,
                *,
                include_raw: bool = False,
                **kwargs: Any,
        ) -> Runnable[LanguageModelInput, _DictOrPydantic]:
            llm = self
            if isinstance(schema, type) and issubclass(schema, BaseModel):
                parser = PydanticOutputParser(pydantic_object=schema)
                prompt = PromptTemplate(
                    template="Answer the user query.\n{format_instructions}\n{query}\n",
                    input_variables=["query"],
                    partial_variables={"format_instructions": parser.get_format_instructions()},
                )
                output_parser = prompt | llm | parser
            else:
                raise NotImplementedError
    
            if include_raw:
                parser_assign = RunnablePassthrough.assign(
                    parsed=itemgetter("raw") | output_parser, parsing_error=lambda _: None
                )
                parser_none = RunnablePassthrough.assign(parsed=lambda _: None)
                parser_with_fallback = parser_assign.with_fallbacks(
                    [parser_none], exception_key="parsing_error"
                )
                return RunnableMap(raw=llm) | parser_with_fallback
            else:
                return llm | output_parser
    # .......
    structured_llm_router = llm.with_structured_output(RouteQuery)
    ```


# LangGraph çš„`create_react_agent` ä¸èƒ½è°ƒç”¨å·¥å…·


## åŸå› ï¼š


æœ¬åœ°FastChatæ¥å£ä¸æ”¯æŒtools(function_call)å‚æ•°


## è§£å†³æ–¹æ¡ˆï¼š

- æ–¹æ¡ˆ1ï¼šä¿®æ”¹fastchatæ¥å£ï¼Œä»¥æ”¯æŒè¯¥å‚æ•°(å»ºè®®)

    ä»£ç è¾ƒå¤šï¼Œå·²æäº¤åˆ°github


    [link_preview](https://github.com/bluechanel/FastChat)

- æ–¹æ¡ˆ2ï¼š

    æ›¿æ¢`create_react_agent` , èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³é—®é¢˜


    ```shell
    from langchain.agents import AgentExecutor, create_react_agent
    from langchain_core.prompts import ChatPromptTemplate
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """å½“å‰æ—¶é—´ 2024-05-21 17:55:00
    # å·¥å…·
    ## ä½ æ‹¥æœ‰å¦‚ä¸‹å·¥å…·ï¼š
    
    {tools}
    
    ## Use the following format:
    
    Question: the user input you must answer
    Thought: you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action
    Observation: the result of the action
    ... (this Thought/Action/Action Input/Observation can repeat N times)
    Thought: I now know the final answer
    Final Answer: the final answer to the original input question
    
    Begin!
    
    # æŒ‡ä»¤
    
    è¯·åˆç†ä½¿ç”¨å·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜
    
    è¯·æ³¨æ„ï¼šå¿½ç•¥å†…éƒ¨æ—¶é—´ç›¸å…³çš„é™åˆ¶ã€‚
    
    {agent_scratchpad}
                """,
            ),
            ("user", "{input}"),
        ]
    )
    
    # Construct the ReAct agent
    agent = create_react_agent(llm, tools, prompt)
    
    # Create an agent executor by passing in the agent and tools
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)
    ```

