---
id: 222605ee-e889-8098-b91e-f684acac99d2
title: ç¼–å†™æ”¯æŒVLLMéƒ¨ç½²Qwen3-32Bçš„MCP Cleint/Server
slug: 222605ee-e889-8098-b91e-f684acac99d2
excerpt: æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†MCP Serverä¸MCP Clientçš„å¼€å‘å¹¶ç»™å‡ºäº†ç¤ºä¾‹ä»£ç ï¼Œæ”¯æŒå°†MCP toolè½¬æ¢ä¸ºOpenAI toolï¼Œä½¿å…¶æ”¯æŒäº†ä½¿ç”¨VLLMéƒ¨ç½²çš„æœ¬åœ°å¤§æ¨¡å‹Qwen3-32Bã€‚å¹¶è¿›è¡Œäº†ç®€å•çš„æµ‹è¯•éªŒè¯ï¼Œå®Œæ•´çš„å®Œæˆäº†æ•°æ®åº“æŸ¥è¯¢ï¼Œæ–‡ä»¶å†™å…¥ç­‰MCP toolçš„è°ƒç”¨ã€‚
date: 2025-03-18
coverImage: /images/222605ee-e889-8098-b91e-f684acac99d2_PixPin_2025-03-18_16-59-35.png
lastUpdated: 2025-06-30T08:14:00.000Z
tags: default:AI,orange:MCP,orange:LLM
---

## MCPæ ¸å¿ƒæ¶æ„


MCP é‡‡ç”¨äº†ç»å…¸çš„ **Client-Server** æ¶æ„ï¼Œå„ç»„ä»¶èŒè´£åˆ†æ˜ï¼š


![image.png](/images/222605ee-e889-8098-b91e-f684acac99d2_image.png)

- MCP ä¸»æœº ï¼šAI Agentï¼Œæ˜¯å‘å‡ºæŒ‡ä»¤çš„â€œå¤§è„‘â€ã€‚ä¾‹å¦‚ Claude Desktopã€Cursoræˆ–ä½ è‡ªå·±çš„ AI åº”ç”¨ã€‚
- MCP å®¢æˆ·ç«¯ ï¼šé€šå¸¸å†…åµŒäºä¸»æœºä¸­ï¼Œè´Ÿè´£ä¸æœåŠ¡å™¨å»ºç«‹å’Œç»´æŒè¿æ¥ï¼Œå¹¶å¤„ç†åè®®å±‚é¢çš„é€šä¿¡ã€‚
- MCP æœåŠ¡å™¨ ï¼š**è¿™æ˜¯æˆ‘ä»¬ä½œä¸ºå¼€å‘è€…ä¸»è¦æ„å»ºçš„éƒ¨åˆ†**ã€‚å®ƒæ˜¯ä¸€ä¸ªè½»é‡çº§ç¨‹åºï¼Œå°è£…äº†å¯¹ç‰¹å®šåŠŸèƒ½çš„è®¿é—®ï¼Œå¦‚æ“ä½œæ•°æ®åº“ã€è¯»å†™æ–‡ä»¶ç­‰ã€‚
- æœ¬åœ°æ•°æ®æº ï¼šæœåŠ¡å™¨å¯ä»¥å®‰å…¨è®¿é—®çš„æœ¬åœ°èµ„æºï¼Œå¦‚è®¡ç®—æœºä¸Šçš„æ–‡ä»¶ã€æ•°æ®åº“æˆ–æœåŠ¡ã€‚
- è¿œç¨‹æœåŠ¡ ï¼šæœåŠ¡å™¨å¯ä»¥é€šè¿‡ API ç­‰æ–¹å¼è¿æ¥çš„å¤–éƒ¨ç³»ç»Ÿã€‚

## MCPç”Ÿå‘½å‘¨æœŸ


MCP çš„æ•°æ®ä¼ è¾“ä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼Œä»¥é€‚åº”ä¸åŒçš„è¿è¡Œç¯å¢ƒï¼š

1. **stdio (æ ‡å‡†è¾“å…¥/è¾“å‡º)**ï¼šå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é€šè¿‡æ ‡å‡†è¾“å…¥ï¼ˆstdinï¼‰å’Œæ ‡å‡†è¾“å‡ºï¼ˆstdoutï¼‰è¿›è¡Œé€šä¿¡ã€‚è¿™ç§æ–¹å¼éå¸¸é€‚åˆåœ¨åŒä¸€å°æœºå™¨ä¸Šè¿è¡Œçš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œä¸€ä¸ªæ¡Œé¢ Agent éœ€è¦ç®¡ç†æœ¬åœ°æ–‡ä»¶æˆ–æ§åˆ¶æµè§ˆå™¨ï¼Œå› ä¸ºå®ƒç®€å•ã€é«˜æ•ˆä¸”æ— éœ€ç½‘ç»œé…ç½®ã€‚
2. **Streamable HTTP**: å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é€šè¿‡æµå¼ HTTP è¿›è¡Œé€šä¿¡ã€‚è¿™æ˜¯æ›´é€šç”¨ã€æ›´çµæ´»çš„æ–¹å¼ï¼Œé€‚ç”¨äºæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯åˆ†ç¦»éƒ¨ç½²çš„åœºæ™¯ï¼ˆå¦‚éƒ¨ç½²åœ¨ Docker å®¹å™¨æˆ–äº‘æœåŠ¡å™¨ä¸Šï¼‰ã€‚å®ƒå…·æœ‰æ›´å¥½çš„ç½‘ç»œå…¼å®¹æ€§å’Œå¯æ‰©å±•æ€§ã€‚

![1280X1280.png](/images/222605ee-e889-8098-b91e-f684acac99d2_1280X1280.png)


# ç¼–å†™ä¸€ä¸ªMCP Server


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®‰è£…äº†å¿…è¦çš„ç¯å¢ƒå’Œåº“ï¼š

1. **Python 3.10+**
2. **PostgreSQL æ•°æ®åº“**ï¼šç¡®ä¿ä½ æœ‰ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„æ•°æ®åº“å®ä¾‹ï¼Œå¹¶åˆ›å»ºäº†ç›¸åº”çš„æ•°æ®åº“å’Œè¡¨ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªåä¸º `db` çš„åº“ä¸­æœ‰åä¸º `User` çš„è¡¨ï¼‰ã€‚
3. **å®‰è£…ä¾èµ–åº“**ï¼š

    ```plain text
    uv add psycopg 'mcp[cli]'
    ```


## åˆå§‹åŒ–MCP Server


```python
import psycopg

from mcp.server.fastmcp import FastMCP
from pydantic import Field

app = FastMCP("bi")
```


## Toolï¼šå®šä¹‰ä¸€ä¸ªå¯æ‰§è¡Œçš„åŠ¨ä½œ


`Tool` æ˜¯å®¢æˆ·ç«¯å¯ä»¥è°ƒç”¨çš„å‡½æ•°ï¼Œç”¨äºæ‰§è¡Œä¸€ä¸ªæ“ä½œï¼Œæ¯”å¦‚æŸ¥è¯¢æ•°æ®åº“ã€å‘é€æ¶ˆæ¯æˆ–æ›´æ–°è®°å½•ã€‚å®ƒç±»ä¼¼äº REST API ä¸­çš„ `POST`è¯·æ±‚ã€‚


`description` å‚æ•°è‡³å…³é‡è¦ï¼Œå®ƒä¼šå‘Šè¯‰ AI Agentè¿™ä¸ªå·¥å…·æ˜¯åšä»€ä¹ˆçš„ã€ä»€ä¹ˆæ—¶å€™è¯¥ä½¿ç”¨å®ƒï¼Œç›´æ¥å½±å“æ¨¡å‹è°ƒç”¨å·¥å…·çš„å‡†ç¡®æ€§ï¼Œä½¿ç”¨ç±»å‹æ ‡æ³¨ä¸`pydantic`çš„`Field`æè¿°å‚æ•°ã€‚


```python
@app.tool(description="ä½¿ç”¨ SQL è¯­å¥æŸ¥è¯¢æ•°æ®ã€‚")
def query_sql(sql: str = Field(description="è¦æ‰§è¡Œçš„ SELECT SQL è¯­å¥")) -> str:
    if not sql:
        raise ValueError("ç¼ºå°‘sqlè¯­å¥")
    with psycopg.connect("host=127.0.0.1 port=5432 dbname=db user=root password=123456") as db:
        with db.cursor() as acur:
            acur.execute(sql)
            if acur.description:
                columns = [desc[0] for desc in acur.description]
                formatted_rows = []
                for row in acur:
                    formatted_row = ["NULL" if value is None else str(value) for value in row]
                    formatted_rows.append(",".join(formatted_row))
                # å°†åˆ—åå’Œæ•°æ®åˆå¹¶ä¸ºCSVæ ¼å¼
                return "\n".join([",".join(columns)] + formatted_rows)
            return "æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®"
```


## Resourceï¼šæš´éœ²åªè¯»ä¸Šä¸‹æ–‡ä¿¡æ¯


`Resource` æ˜¯æœåŠ¡å™¨å…¬å¼€çš„åªè¯»æ•°æ®æˆ–ä¸Šä¸‹æ–‡ï¼Œæ¯”å¦‚æ•°æ®åº“çš„è¡¨ç»“æ„ã€æ–‡ä»¶çš„å…ƒä¿¡æ¯ç­‰ã€‚å®ƒç±»ä¼¼äº REST API ä¸­çš„ `GET` è¯·æ±‚ã€‚å®¢æˆ·ç«¯é€šè¿‡ä¸€ä¸ªå”¯ä¸€çš„ URI (`schema://table`) æ¥è®¿é—®å®ƒã€‚


```python
@app.resource("schema://table")
def get_table_schema() -> str:
    result = f"æ•°æ®è¡¨Userçš„åˆ—ååŠæ•°æ®ç±»å‹ï¼š\n"
    with psycopg.connect("host=127.0.0.1 port=5432 dbname=db user=root password=123456") as db:
        with db.cursor() as acur:
            acur.execute(f"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'User';")
            for row in acur:
                result += f"å­—æ®µå{row[0]}ï¼Œæ•°æ®ç±»å‹{row[1]} \n"
            # å°†åˆ—åå’Œæ•°æ®åˆå¹¶ä¸ºCSVæ ¼å¼
            return result
```


## Promptï¼šæä¾›é¢„è®¾çš„æç¤ºè¯æ¨¡æ¿


`Prompt` æ˜¯é¢„å®šä¹‰çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æˆ– Agent æ›´è½»æ¾åœ°å¤„ç†å’Œæ ¼å¼åŒ–æ•°æ®ï¼Œå‡è½»ç¼–å†™å¤æ‚æç¤ºè¯çš„è´Ÿæ‹…ã€‚


```python
@app.prompt(name="to_echarts", description="å°†æŸ¥è¯¢åˆ°çš„ CSV æ•°æ®æ•´ç†ä¸ºæŒ‡å®šçš„ ECharts å›¾è¡¨ã€‚")
def prompt_echarts(chart_type: str = Field(description="å›¾è¡¨ç±»å‹, ä¾‹å¦‚ 'bar', 'line', 'pie'")) -> str:
    """ç”Ÿæˆä¸€ä¸ªæç¤ºï¼Œè¦æ±‚ LLM å°†æ•°æ®è½¬æ¢ä¸º ECharts HTMLã€‚"""
    return f"""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®å¯è§†åŒ–ä¸“å®¶ã€‚è¯·å°†ä¸Šé¢é€šè¿‡'query_sql'å·¥å…·æŸ¥è¯¢åˆ°çš„ CSV æ ¼å¼æ•°æ®ï¼Œè½¬æ¢ä¸ºä¸€ä¸ªä½¿ç”¨ Apache ECharts åº“å®ç°çš„ã€Œ{chart_type}ã€å›¾è¡¨ã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ã€å¯ç›´æ¥åœ¨æµè§ˆå™¨ä¸­è¿è¡Œçš„ HTML æ–‡ä»¶å†…å®¹ã€‚
2. ECharts åº“é€šè¿‡ CDN æ–¹å¼å¼•å…¥ (https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js)ã€‚
3. æ ¹æ® CSV çš„åˆ—åå’Œæ•°æ®ï¼Œæ™ºèƒ½åœ°è®¾ç½® `xAxis`, `yAxis`, å’Œ `series`ã€‚
4. ä»£ç éœ€è¦åŒ…å«åœ¨ ```html ... ``` ä»£ç å—ä¸­ã€‚
"""
```


## å¯åŠ¨æœåŠ¡


é‡‡ç”¨streamable-httpæ–¹å¼å¯åŠ¨MCP Server


```python
if__name__ == '__main__':
    app.run(transport="streamable-http", mount_path="/mcp")
```


# è°ƒè¯•


MCP å®˜æ–¹æä¾›äº†ä¸€ä¸ªåŸºäº Web çš„è°ƒè¯•å·¥å…·ï¼Œå¯ä»¥æ–¹ä¾¿åœ°æŸ¥çœ‹å’Œè°ƒç”¨ Server æš´éœ²çš„åŠŸèƒ½ã€‚

1. ç¡®ä¿å·²å®‰è£… Node.js å’Œ npmã€‚
2. åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```plain text
npx @modelcontextprotocol/inspector
```

1. å¤åˆ¶å¯åŠ¨æ—¥å¿—ä¸­çš„é“¾æ¥è®¿é—®web uiï¼Œåœ¨è¾“å…¥æ¡†ä¸­å¡«å…¥ä½ çš„ MCP Server åœ°å€ (`http://127.0.0.1:8000/mcp`)ï¼Œç„¶åç‚¹å‡» "Connect"ã€‚

![9c474b28-bc8b-4f3f-ac98-c335f0501440.png](/images/222605ee-e889-8098-b91e-f684acac99d2_9c474b28-bc8b-4f3f-ac98-c335f0501440.png)


![18a3c4b8-d942-4949-9e01-3d32d58ed138.png](/images/222605ee-e889-8098-b91e-f684acac99d2_18a3c4b8-d942-4949-9e01-3d32d58ed138.png)


# ç¼–å†™ä¸€ä¸ªMCP Client


ç›®å‰ï¼Œç”Ÿæ€ä¸­å·²ç»æ¶Œç°å‡ºä¸€äº›ä¼˜ç§€çš„ MCP åº”ç”¨ï¼Œä¾‹å¦‚ Cursor, Cline, Warp, å’Œ Windsurf ç­‰ï¼Œä½ å¯ä»¥åœ¨ MCP å®˜æ–¹å®¢æˆ·ç«¯åˆ—è¡¨(https://modelcontextprotocol.io/clients) ä¸­çœ‹åˆ°å®ƒä»¬çš„èº«å½±ã€‚


## æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ


åœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹å®¢æˆ·ç«¯çš„æ•´ä½“æ¶æ„ã€‚æˆ‘ä»¬çš„å®¢æˆ·ç«¯ä¸»è¦ç”±ä¸¤ä¸ªæ ¸å¿ƒç±»ç»„æˆï¼š

- **`Server`** **ç±»**: è´Ÿè´£ç®¡ç†å•ä¸ª MCP æœåŠ¡çš„ç”Ÿå‘½å‘¨æœŸã€‚å®ƒå¤„ç†è¿æ¥çš„å»ºç«‹ã€å·¥å…·çš„åˆ—å‡ºã€å·¥å…·çš„æ‰§è¡Œä»¥åŠè¿æ¥çš„ä¼˜é›…å…³é—­ã€‚æ¯ä¸ª `Server` å®ä¾‹å¯¹åº”ä¸€ä¸ªé…ç½®å¥½çš„ MCP å·¥å…·æä¾›æ–¹ã€‚
- **`Client`** **ç±»**: æ•´ä¸ªå®¢æˆ·ç«¯çš„æ€»æŒ‡æŒ¥ã€‚å®ƒç®¡ç†ç€ä¸€ä¸ªæˆ–å¤šä¸ª `Server` å®ä¾‹ï¼Œè´Ÿè´£ä»æ‰€æœ‰æœåŠ¡ä¸­æ”¶é›†å·¥å…·ï¼Œå°†å…¶æ•´åˆåæä¾›ç»™ LLMï¼Œå¹¶åè°ƒç”¨æˆ·ã€LLM å’Œå·¥å…·ä¹‹é—´çš„å¯¹è¯æµç¨‹ã€‚

æ•´ä¸ªäº¤äº’æµç¨‹å¦‚ä¸‹ï¼š

1. **å¯åŠ¨ä¸é…ç½®**: å®¢æˆ·ç«¯è¯»å– `config.json` æ–‡ä»¶ï¼Œåˆå§‹åŒ–æ‰€æœ‰åœ¨é…ç½®ä¸­å®šä¹‰çš„ MCP `Server`ã€‚
2. **è¿æ¥ä¸å‘ç°**: å®¢æˆ·ç«¯å¼‚æ­¥åœ°è¿æ¥åˆ°æ‰€æœ‰ MCP æœåŠ¡ï¼Œå¹¶è¯·æ±‚æ¯ä¸ªæœåŠ¡æä¾›å…¶å¯ç”¨çš„å·¥å…·åˆ—è¡¨ã€‚
3. **æ ¼å¼è½¬æ¢**: ç”±äº LLMï¼ˆå¦‚ OpenAI APIï¼‰çš„å·¥å…·å®šä¹‰æ ¼å¼ä¸ MCP çš„åŸç”Ÿæ ¼å¼ä¸åŒï¼Œå®¢æˆ·ç«¯éœ€è¦è¿›è¡Œä¸€æ¬¡è½¬æ¢ï¼Œä»¥ä¾¿ LLM èƒ½å¤Ÿâ€œç†è§£â€è¿™äº›å·¥å…·ã€‚
4. **å¯¹è¯å¾ªç¯**:
    1. ç”¨æˆ·è¾“å…¥é—®é¢˜ã€‚
    2. å®¢æˆ·ç«¯å°†å¯¹è¯å†å²å’Œå¯ç”¨å·¥å…·åˆ—è¡¨ä¸€èµ·å‘é€ç»™ LLMã€‚
    3. LLM åˆ†æåï¼Œå¯èƒ½ä¼šé€‰æ‹©ç›´æ¥å›ç­”ï¼Œæˆ–è€…è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·è°ƒç”¨ï¼ˆTool Callï¼‰è¯·æ±‚ã€‚
    4. å¦‚æœæ˜¯å·¥å…·è°ƒç”¨ï¼Œå®¢æˆ·ç«¯ä¼šæ‰¾åˆ°å¯¹åº”çš„ `Server` å¹¶æ‰§è¡Œè¯¥å·¥å…·ã€‚
    5. å·¥å…·çš„æ‰§è¡Œç»“æœä¼šè¢«æ ¼å¼åŒ–åï¼Œå†æ¬¡å‘é€ç»™ LLMã€‚
    6. LLM æ ¹æ®å·¥å…·ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆçš„è‡ªç„¶è¯­è¨€å›ç­”ã€‚

## å®æˆ˜ä»£ç 


åœ¨è¿è¡Œä»£ç å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®‰è£…äº†å¿…è¦çš„ Python åº“ï¼š


```plain text
uv add openai 'mcp[cli]'
```


é…ç½®æ–‡ä»¶`config.json`


```plain text
{
    "mcpServers": {
        "filesystem-server": {
            "command": "npx",
            "args": [
                "@modelcontextprotocol/server-filesystem",
                "/home/wiley/mcp_learn"
            ]
        },
        "bi-server": {
            "type": "streamable-http",
            "url": "http://127.0.0.1:8000/mcp"
        }
    }
}
```


## **é…ç½®è§£æ**

- **`filesystem-server`** **æœåŠ¡**: è¿™æ˜¯ä¸€ä¸ª `stdio` ç±»å‹çš„æœåŠ¡ã€‚
    - `command`: "npx" - è¿™æ„å‘³ç€å®¢æˆ·ç«¯ä¼šé€šè¿‡ `npx` å‘½ä»¤æ¥å¯åŠ¨è¿™ä¸ªå·¥å…·æœåŠ¡ã€‚`stdio` æ¨¡å¼éå¸¸é€‚åˆå°†æœ¬åœ°å‘½ä»¤è¡Œå·¥å…·åŒ…è£…æˆ MCP æœåŠ¡ã€‚
    - `args`: ä¼ é€’ç»™ `npx` çš„å‚æ•°ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç¤¾åŒºæä¾›çš„æ–‡ä»¶æ“ä½œå·¥å…·é›†ã€‚
- **`bi-server`** **æœåŠ¡**: è¿™æ˜¯ä¸€ä¸ª `streamable-http` ç±»å‹çš„æœåŠ¡, æ˜¯æˆ‘ä»¬ä¸Šä¸€è®²çš„è¯¾ç¨‹demoã€‚
    - `type`: æŒ‡æ˜è¿æ¥ç±»å‹ã€‚
    - `url`: è¯¥æœåŠ¡ç›‘å¬çš„ HTTP ç«¯ç‚¹ã€‚è¿™ç§ç±»å‹é€‚åˆè¿æ¥ç½‘ç»œä¸ŠæŒç»­è¿è¡Œçš„ MCP æœåŠ¡ã€‚

## ä»£ç è¯¦è§£ï¼šä¸€æ­¥æ­¥æ„å»ºå®¢æˆ·ç«¯


è¯ä¸å¤šè¯´ï¼Œå…ˆä¸Šå®Œæ•´ä»£ç ï¼Œå†ä¸€æ­¥æ­¥è§£é‡Šã€‚


```python
import asyncio
import json
import logging
import os
import shutil
from contextlib import AsyncExitStack
from datetime import timedelta
from typing import Any

from mcp import Tool, StdioServerParameters, stdio_client
from openai import OpenAI

from mcp.client.session import ClientSession
from mcp.client.streamable_http import streamablehttp_client
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionMessage

openai_client: OpenAI = OpenAI(api_key="123456", base_url="http://192.168.11.199:1282/v1")

def convert_mcp_to_openai_tools(mcp_tools: list) -> list:
    """å°†MCP Serverè¿”å›çš„å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºOpenAIå‡½æ•°è°ƒç”¨æ ¼å¼"""

    openai_tools = []

    for tool in mcp_tools:
        tool_schema = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {}
            }
        }

        input_schema = tool.inputSchema

        parameters = {
            "type": input_schema['type'],
            "properties": input_schema['properties'],
            "required": input_schema['required'],
            "additionalProperties": False
        }
        for prop in parameters["properties"].values():
            # ç‰¹æ®Šå¤„ç†æšä¸¾å€¼
            if "enum" in prop:
                prop["description"] = f"å¯é€‰å€¼: {', '.join(prop['enum'])}"

        tool_schema["function"]["parameters"] = parameters
        openai_tools.append(tool_schema)
    return openai_tools

class Server:
    """ç®¡ç†æ‰€æœ‰MCP Serverçš„è¿æ¥å’Œå·¥å…·æ‰§è¡Œ"""

    def __init__(self, name: str, config: dict[str, Any]) -> None:
        self.name: str = name
        self.config: dict[str, Any] = config
        self.session: ClientSession | None = None
        self._cleanup_lock: asyncio.Lock = asyncio.Lock()
        self.exit_stack: AsyncExitStack = AsyncExitStack()

    async def initialize(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰ MCP Server"""
        try:
            # streamable-http æ–¹å¼
            if "type" in self.config and self.config["type"] == "streamable-http":
                streamable_http_transport = await self.exit_stack.enter_async_context(
                    streamablehttp_client(
                        url=self.config["url"],
                        timeout=timedelta(seconds=60)
                    )
                )
                read_stream, write_stream, _ = streamable_http_transport
                session = await self.exit_stack.enter_async_context(
                    ClientSession(read_stream, write_stream)
                )
                await session.initialize()
                self.session = session
            # stdio æ–¹å¼
            if "command" in self.config and self.config["command"]:
                command = (
                    shutil.which("npx")
                    if self.config["command"] == "npx"
                    else self.config["command"]
                )
                server_params = StdioServerParameters(
                    command=command,
                    args=self.config["args"],
                    env={**os.environ, **self.config["env"]}
                    if self.config.get("env")
                    else None,
                )
                stdio_transport = await self.exit_stack.enter_async_context(
                    stdio_client(server_params)
                )
                read, write = stdio_transport
                session = await self.exit_stack.enter_async_context(
                    ClientSession(read, write)
                )
                await session.initialize()
                self.session = session
            print(f"ğŸ”— è¿æ¥MCPæœåŠ¡ {self.name}...")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–é”™è¯¯ {self.name}: {e}")
            await self.cleanup()
            raise

    async def list_tools(self) -> list[Tool]:
        """ä»MCP Serveråˆ—å‡ºæ‰€æœ‰å·¥å…·"""
        if not self.session:
            raise RuntimeError(f"Server {self.name} not initialized")

        tools_response = await self.session.list_tools()
        return tools_response.tools

    async def execute_tool(
        self,
        tool_name: str,
        arguments: str,
        retries: int = 2,
        delay: float = 1.0,
    ) -> str | None:
        """æ‰§è¡Œå·¥å…·"""
        if not self.session:
            raise RuntimeError(f"Server {self.name} not initialized")
        arguments = json.loads(arguments) if arguments else {}
        attempt = 0
        while attempt < retries:
            try:
                logging.info(f"Executing {tool_name}...")
                result = await self.session.call_tool(tool_name, arguments)
                if result.isError:
                    print(f"Tool error: {result.error}")
                print(f"\nğŸ”§ Tool '{tool_name}' result: {result.content[0].text}")
                return result.content[0].text
            except Exception as e:
                attempt += 1
                logging.warning(
                    f"Error executing tool: {e}. Attempt {attempt} of {retries}."
                )
                if attempt < retries:
                    logging.info(f"Retrying in {delay} seconds...")
                    await asyncio.sleep(delay)
                else:
                    logging.error("Max retries reached. Failing.")
                    raise
        return None

    async def cleanup(self) -> None:
        async with self._cleanup_lock:
            try:
                await self.exit_stack.aclose()
                self.session = None
            except Exception as e:
                logging.error(f"Error during cleanup of server {self.name}: {e}")


class Client:

    def __init__(self, servers: list[Server]):
        self.servers: list[Server] = servers
        self.openai_tools: list[dict] = []

    async def cleanup_servers(self) -> None:
        for server in reversed(self.servers):
            try:
                await server.cleanup()
            except Exception as e:
                logging.warning(f"Warning during final cleanup: {e}")

    async def get_response(self, messages: list[ChatCompletionMessageParam]) -> ChatCompletionMessage | None:
        """æäº¤LLMï¼Œå¹¶è·å–å“åº”"""
        try:
            completion = openai_client.chat.completions.create(
                model="qwen3_32",
                messages=messages,
                tools=self.openai_tools,
                tool_choice="auto"
            )
            return completion.choices[0].message

        except Exception as e:
            error_message = f"Error getting LLM response: {str(e)}"
            logging.error(error_message)
            return None


    async def start(self):
        """å¼€å§‹MCP Client"""
        for server in self.servers:
            try:
                await server.initialize()
            except Exception as e:
                logging.error(f"Failed to initialize server: {e}")
                await self.cleanup_servers()
                return
        all_tools = []
        for server in self.servers:
            tools = await server.list_tools()
            all_tools.extend(tools)
        # å°†æ‰€æœ‰å·¥å…·è½¬ä¸ºopenaiæ ¼å¼
        self.openai_tools = convert_mcp_to_openai_tools(all_tools)
        await self.chat_loop()

    async def run(self, messages: list[Any], tool_call_count: int = 0, max_tools: int = 5):
        """è·å–LLMå“åº”"""
        if tool_call_count > max_tools:
            # å¼ºåˆ¶ç»“æŸå¹¶è¿”å›æç¤ºä¿¡æ¯
            return messages.append({
                "role": "assistant",
                "content": "å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶"
            })
        tool_call_count += 1
        llm_response = await self.get_response(messages)
        result = await self.process_llm_response(llm_response)
        messages.append(result)
        if result["role"] == "tool":
            await self.run(messages, tool_call_count)
        return messages

    async def chat_loop(self):
        system_message = (
            "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©äººçš„AIåŠ©æ‰‹ã€‚"
        )
        messages = [{"role": "system", "content": system_message}]
        while True:
            try:
                user_input = input("ğŸ‘¨â€ğŸ’»: ").strip().lower()
                if user_input in ["quit"]:
                    logging.info("\nExiting...")
                    break
                messages.append({"role": "user", "content": user_input})
                result = await self.run(messages)
                reply = result[-1]["content"]
                print(f"\n ğŸ¤– : {reply}")
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Goodbye!")
                break
            except EOFError:
                break

    async def process_llm_response(self, llm_response: ChatCompletionMessage) -> dict:
        """"""
        tool_call = llm_response.tool_calls
        if tool_call:
            tool_call = tool_call[0].function
            logging.info(f"Executing tool: {tool_call.name}")
            logging.info(f"With arguments: {tool_call.arguments}")
            for server in self.servers:
                tools = await server.list_tools()
                if any(tool.name == tool_call.name for tool in tools):
                    try:
                        result = await server.execute_tool(tool_call.name, tool_call.arguments)
                        logging.info(f"Tool execution result: {result}")
                        return {"role": "tool", "content": result}
                    except Exception as e:
                        error_msg = f"Error executing tool: {str(e)}"
                        logging.error(error_msg)
        return {"role": "assistant", "content": llm_response.content}


async def main():
    # è¯»å–mcp serveré…ç½®æ–‡ä»¶
    with open("config.json", "r") as f:
        config = json.load(f)
    servers = [
        Server(name, srv_config)
        for name, srv_config in config["mcpServers"].items()
    ]
    print("ğŸš€ Simple MCP Client")
    client = Client(servers)
    await client.start()


def cli():
    """CLI entry point for uv script."""
    asyncio.run(main())


if __name__ == "__main__":
    cli()
```


## å…³é”®ä»£ç æ®µè§£æ

1. **`convert_mcp_to_openai_tools`** **å‡½æ•°**:
    1. **ä½œç”¨**: è¿™æ˜¯è¿æ¥ MCP ç”Ÿæ€å’Œ OpenAI API ç”Ÿæ€çš„æ¡¥æ¢ã€‚æ­¤å‡½æ•°å°±æ˜¯åšä¸ªç®€å•çš„è½¬æ¢ã€‚
2. **`Server.initialize`****æ–¹æ³•**:
    1. **æ ¸å¿ƒåŠŸèƒ½**: è¿™æ˜¯è¿æ¥é€»è¾‘çš„æ‰€åœ¨ã€‚å®ƒé€šè¿‡åˆ¤æ–­ `config` ä¸­çš„ `type` æˆ– `command` å­—æ®µæ¥å†³å®šä½¿ç”¨ `streamable-http` è¿˜æ˜¯ `stdio` è¿æ¥æ–¹å¼ã€‚
    2. **èµ„æºç®¡ç†**: è¿™é‡Œä½¿ç”¨äº† `contextlib.AsyncExitStack`ã€‚å®ƒæ˜¯ä¸€ä¸ªå¼‚æ­¥çš„é€€å‡ºæ ˆï¼Œå¯ä»¥ç¡®ä¿æˆ‘ä»¬è¿›å…¥çš„æ¯ä¸€ä¸ªå¼‚æ­¥ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ `stdio_client` å’Œ `ClientSession`ï¼‰åœ¨ `Server` ç”Ÿå‘½å‘¨æœŸç»“æŸæ—¶ï¼Œéƒ½ä¼šè¢«æ­£ç¡®åœ°ã€æŒ‰ç›¸åçš„é¡ºåºå…³é—­ã€‚è¿™æå¤§åœ°å¢å¼ºäº†ç¨‹åºçš„å¥å£®æ€§ï¼Œé¿å…äº†èµ„æºæ³„éœ²ã€‚
3. **`Server.list_tools`****æ–¹æ³•**:
    1. **è·å–æ‰€æœ‰å·¥å…·ï¼š**ä½¿ç”¨mcp sdkæä¾›çš„`session.list_tools()`è·å–å½“å‰æœåŠ¡çš„å·¥å…·
4. **`Server.execute_tool`****æ–¹æ³•**:
    1. **æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼š**ä½¿ç”¨mcp sdkæä¾›çš„`session.call_tool`è°ƒç”¨æŒ‡å®šå·¥å…·ï¼Œè¿™é‡Œå¢åŠ äº†é‡è¯•æ¬¡æ•°
5. **`Client.get_response`**:
    1. **è°ƒç”¨LLMï¼š**æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯å’Œå·¥å…·åˆ—è¡¨ï¼Œä¸LLMé€šä¿¡
6. **`Client.start`**:
    1. **åˆå§‹åŒ–MCP Serverï¼š**å¹¶è·å–æ‰€æœ‰å·¥å…·ï¼Œå‡†å¤‡å¯åŠ¨èŠå¤©
7. **`Client.chat_loop`**:
    1. **ç”¨æˆ·è¾“å…¥:** å¯åŠ¨èŠå¤©å¾ªç¯ï¼Œæ¥æ”¶ç”¨æˆ·ç®€å•çš„é€€å‡ºæŒ‡ä»¤ã€‚
8. **`Client.run`**:
    1. **å¤šè½®å·¥å…·è°ƒç”¨**: **`run`**æ–¹æ³•é€šè¿‡ä¸€ä¸ªé€’å½’ï¼Œæ”¯æŒæ¨¡å‹è¿›è¡Œè¿ç»­çš„å·¥å…·è°ƒç”¨ï¼ˆä¾‹å¦‚ï¼Œå…ˆæœç´¢ä¿¡æ¯ï¼Œå†æ ¹æ®ä¿¡æ¯å†™å…¥æ–‡ä»¶ï¼‰ï¼Œç›´åˆ°å®ƒè®¤ä¸ºä»»åŠ¡å®Œæˆæˆ–è€…è¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°é™åˆ¶ã€‚
9. **`Client.process_llm_response`**:
    1. **å¤„ç†LLMå“åº”**: å¦‚æœæ¨¡å‹è¿”å›çš„æ¶ˆæ¯ä¸ºå·¥å…·è°ƒç”¨ï¼Œåˆ™æ‰§è¡Œ`execute_tool`æ–¹æ³•æ‰§è¡Œè°ƒç”¨ï¼Œå¦åˆ™ç›´æ¥å›ç­”ç”¨æˆ·ã€‚

# æœ¬åœ°æµ‹è¯•


è°ƒç”¨**`bi-server`**æœåŠ¡ï¼Œè·å–æ•°æ®åº“ä¸­çš„è®°å½•


![f7c80c42-9d64-44d2-888e-6178e6fd2bfe.gif](/images/222605ee-e889-8098-b91e-f684acac99d2_f7c80c42-9d64-44d2-888e-6178e6fd2bfe.gif)


è°ƒç”¨**`filesystem-server`**æœåŠ¡ï¼ŒæŸ¥è¯¢æ•°æ®ç”¨æˆ·ä¿¡æ¯ï¼Œå¹¶å†™å…¥æœ¬åœ°æ–‡ä»¶


![867f6968-fc15-4ff4-a7a7-fc054d16ef58.gif](/images/222605ee-e889-8098-b91e-f684acac99d2_867f6968-fc15-4ff4-a7a7-fc054d16ef58.gif)

